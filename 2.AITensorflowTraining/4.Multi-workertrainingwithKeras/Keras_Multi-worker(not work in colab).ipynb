{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Keras_Multi-worker.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNz3JrOko/74Erh8MBw0/T3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Keras 를 사용한 Multi-worker 훈련\n","tf.distribute.Strategy API 를 사용하여 케라스 모델을 다중 워커로 분산 훈련하는 방법을 살펴봅니다.<BR>\n","tf.distribute.Strategy API 에 관한 내용은\n","[텐서플로로 분산 훈련하기](https://www.tensorflow.org/guide/distributed_training?hl=ko) 를 참고해주세요."],"metadata":{"id":"DQevHm8mkZyZ"}},{"cell_type":"markdown","source":["## 설정"],"metadata":{"id":"I6cZ4aPJk7qL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CskTNDjckX0Y"},"outputs":[],"source":["from __future__ import absolute_import, division, print_function, unicode_literals"]},{"cell_type":"code","source":["try:\n","  # %tensorflow_version 기능은 코랩에서만 사용할 수 있습니다.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","tfds.disable_progress_bar()"],"metadata":{"id":"wRaKt3Kjk_aK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset 준비"],"metadata":{"id":"NPQ6_r5DlCzs"}},{"cell_type":"code","source":["BUFFER_SIZE = 10000\n","BATCH_SIZE = 64\n","\n","# MNIST 데이터를 (0, 255] 범위에서 (0., 1.] 범위로 조정\n","def scale(image, label):\n","  image = tf.cast(image, tf.float32)\n","  image /= 255\n","  return image, label\n","\n","datasets, info = tfds.load(name='mnist',\n","                           with_info=True,\n","                           as_supervised=True)\n","\n","train_datasets_unbatched = datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)\n","train_datasets = train_datasets_unbatched.batch(BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CA13QY_glFvl","executionInfo":{"status":"ok","timestamp":1640670519700,"user_tz":-540,"elapsed":1732,"user":{"displayName":"김성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14454204284131910272"}},"outputId":"f7e237a6-7d7b-472d-e45e-249067bcb3ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /root/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n","local data directory. If you'd instead prefer to read directly from our public\n","GCS bucket (recommended if you're running on GCP), you can instead pass\n","`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n","\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"]}]},{"cell_type":"markdown","source":["## Keras 모델 만들기"],"metadata":{"id":"TYC6qSRDlHqo"}},{"cell_type":"code","source":["def build_and_compile_cnn_model():\n","  model = tf.keras.Sequential([\n","      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n","      tf.keras.layers.MaxPooling2D(),\n","      tf.keras.layers.Flatten(),\n","      tf.keras.layers.Dense(64, activation='relu'),\n","      tf.keras.layers.Dense(10, activation='softmax')\n","  ])\n","  model.compile(\n","      loss=tf.keras.losses.sparse_categorical_crossentropy,\n","      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n","      metrics=['accuracy'])\n","  return model"],"metadata":{"id":"zMTh1gmulHi-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 단일 워커로 모델 테스트"],"metadata":{"id":"YcYOunR0lQk1"}},{"cell_type":"code","source":["single_worker_model = build_and_compile_cnn_model()\n","single_worker_model.fit(x=train_datasets, epochs=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-dLEJ6HRlP0t","executionInfo":{"status":"ok","timestamp":1640670633617,"user_tz":-540,"elapsed":108605,"user":{"displayName":"김성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14454204284131910272"}},"outputId":"dc1c66cf-032f-4b4c-c5b6-56ea56f6b349"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","938/938 [==============================] - 36s 36ms/step - loss: 2.0822 - accuracy: 0.4690\n","Epoch 2/3\n","938/938 [==============================] - 26s 27ms/step - loss: 1.2276 - accuracy: 0.7609\n","Epoch 3/3\n","938/938 [==============================] - 26s 28ms/step - loss: 0.6422 - accuracy: 0.8479\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f91fee0d250>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## Multi-worker 구성\n","텐서플로에서 여러 장비를 사용할 때는 TF_CONFIG 환경 변수를 설정해야 합니다. 하나의 클러스터를 구성하는 각 장비에 클러스터 구성을 알려주고 각각 다른 역할을 부여할 수 있습니다.<BR>\n","- cluster : worker 같은 여러 타입의 작업 이름을 키로 하는 파이썬 딕셔너리로 훈련 클러스터에 대한 정보를 지정합니다. 이 중에 체크포인트를 저장하거나, 서머리를 쓰는 일 등을 추가로 담당하는 워커가 필요한데 이를 Chief 워커라고 합니다. 관례적으로 index 번호가 0인 워커가 치프워커가 됩니다.\n","- task : 현재 워커의 작업에 대한 정보를 지정합니다."],"metadata":{"id":"tJL5Mo_ClZ2z"}},{"cell_type":"markdown","source":["### TF_CONFIG 예시\n","두개의 워커를 localhost 에 띄우는 예시입니다. 실제로는 각 워커를 다른 장비에서 띄울텐데, 실제 IP 주소와 포트를 할당하고, 그에 맞게 TF_CONFIG 를 지정해야 합니다.<BR>\n","(주의. 해당 코드는 코랩에서는 실행하면 안됩니다. 주어진 IP와 포트로 gRPC 서버를 띄우려할텐데, 아마도 실패할 것입니다.)"],"metadata":{"id":"FgNaccBTmVEe"}},{"cell_type":"code","source":["os.environ['TF_CONFIG'] = json.dumps({\n","    'cluster': {\n","        # 'worker': [\"13.124.188.202:12345\", \"3.37.345.50:23456\"]\n","        'worker': [\"localhost:12345\", \"localhost:23456\"]\n","    },\n","    'task': {'type': 'worker', 'index': 0}\n","})"],"metadata":{"id":"Yxnz0f2KlZrh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 전략 선택\n","MultiWorkerMirroredStrategy 는 동기 다중 워커 훈련에서 추천하는 전략으로 모델의 레이어에 있는 모든 변수으 ㅣ복사본을 각 워커의 장치마다 만들어서 CollectiveOps 를 사용하여 그래디언트를 모으고, 각 변수의 값을 동기화합니다.<BR>\n","더 자세한 내용은 [tf.distribute.Strategy 가이드](https://www.tensorflow.org/guide/distributed_training?hl=ko)를 참고하세요."],"metadata":{"id":"TTD2patdrJ2p"}},{"cell_type":"code","source":["strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"],"metadata":{"id":"Nk4x0vomrKXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 훈련\n","MultiWorkerMirroredStrategy 를 Keras 와 함께 사용하려면 모델 구성과 compile() 호출 코드를 strategy.scope() 안으로 넣어주면 됩니다."],"metadata":{"id":"9z7ZbIjarwza"}},{"cell_type":"code","source":["NUM_WORKERS = 2\n","# 여기서 배치 크기는 워커의 수를 곱한 크기로 늘려야 합니다. `tf.data.Dataset.batch`에는\n","# 전역 배치 크기를 지정해야 하기 때문입니다. 전에는 64였지만, 이제 128이 됩니다.\n","GLOBAL_BATCH_SIZE = 64 * NUM_WORKERS\n","train_datasets = train_datasets_unbatched.batch(GLOBAL_BATCH_SIZE)\n","with strategy.scope():\n","  multi_worker_model = build_and_compile_cnn_model()\n","multi_worker_model.fit(x=train_datasets, epochs=3)"],"metadata":{"id":"cMpOow-2rwom"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터셋 샤딩과 배치 크기\n","tfdistribute.Strategy API가 다중워커 훈련에 맞게 자동으로 데이터셋을 샤딩해줍니다.<BR>\n","만약에 직접 샤딩을 하고 싶다면 다음과 같이 자동 샤딩 기능을 끌 수 있습니다."],"metadata":{"id":"sgv4sdPEsNIt"}},{"cell_type":"code","source":["options = tf.data.Options()\n","options.experimental_distribute.auto_shard = False\n","train_datasets_no_auto_shard = train_datasets.with_options(options)"],"metadata":{"id":"JeKp-YGfsM1t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 성능\n","MultiWorkerMirroredStrategy 를 사용하여 여러 워커를 사용하여 훈련할 수 있습니다.<BR>\n","가능하면 변수를 tf.float 타입으로 바꾸십시오.<BR>\n","[공식 ResNet 모델 예제](https://github.com/tensorflow/models/blob/8367cf6dabe11adf7628541706b660821f397dce/official/resnet/resnet_model.py#L466)"],"metadata":{"id":"Ozkp9HANHjVY"}},{"cell_type":"markdown","source":["## ModelCheckpoint 콜백\n","다중 워커 훈련의 내결함 기능을 사용하려면, Model.fit() 을 호출할 때 ModelCheckpoint 의 인스턴스를 제공해야 합니다."],"metadata":{"id":"V39XzxukIHII"}},{"cell_type":"code","source":["# `filepath` 매개변수를 모든 워커가 접근할 수 있는 파일 시스템 경로로 바꾸십시오.\n","callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='/tmp/keras-ckpt')]\n","with strategy.scope():\n","  multi_worker_model = build_and_compile_cnn_model()\n","multi_worker_model.fit(x=train_datasets, epochs=3, callbacks=callbacks)"],"metadata":{"id":"S7f7kD33HjDh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 워커가 정지당하면, 정지당한 워커가 다시 살아날 때까지 전체 클러스터가 잠시 멈추고, 워커가 다시 들어오면 다른 워커도 재시작 됩니다. 모든 워커가 이전에 저장한 체크포인트 파일을 읽고, 예전 상태를 불러오면 클러스터가 다시 일관된 상태가 됩니다."],"metadata":{"id":"9Tk44gaxIanQ"}}]}